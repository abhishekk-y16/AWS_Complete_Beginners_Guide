# DynamoDB Use Cases ğŸ¯

Real-world scenarios where DynamoDB excels.

## Use Case 1: Real-Time Web Application

### Scenario

```
You're building: E-commerce shopping cart

Requirements:
â”œâ”€ Store shopping carts (temporary, 24-hour TTL)
â”œâ”€ Update quantity instantly
â”œâ”€ Get cart for specific user
â”œâ”€ Handle 1000s of concurrent users
â”œâ”€ Variable traffic (spikes on weekends)
â””â”€ Must be fast (< 100ms)

Why DynamoDB:
â”œâ”€ Partition by user_id (each user separate)
â”œâ”€ TTL: Auto-delete abandoned carts (24h)
â”œâ”€ On-demand pricing: Pay for actual usage
â”œâ”€ Millisecond response times
â””â”€ Handles traffic spikes automatically
```

### Data Model

```
Table: ShoppingCarts
â”œâ”€ Partition Key: user_id
â”œâ”€ Sort Key: session_id
â”œâ”€ TTL: expires_at (auto-delete after 24h)
â””â”€ Attributes:
   â”œâ”€ items (List)
   â”‚  â”œâ”€ {product_id, qty, price}
   â”‚  â”œâ”€ {product_id, qty, price}
   â”‚  â””â”€ {...}
   â”œâ”€ total: 199.99
   â”œâ”€ created_at: 1706461200
   â””â”€ updated_at: 1706470000
```

### Operations

```javascript
// 1. Add item to cart
await dynamodb.updateItem({
  TableName: "ShoppingCarts",
  Key: { user_id: "user-100", session_id: "sess-xyz" },
  UpdateExpression: "SET items = list_append(items, :item), updated_at = :now",
  ExpressionAttributeValues: {
    ":item": [{product_id: "prod-123", qty: 1, price: 49.99}],
    ":now": Date.now()
  }
});

// 2. Get cart
const cart = await dynamodb.getItem({
  TableName: "ShoppingCarts",
  Key: { user_id: "user-100", session_id: "sess-xyz" }
});

// 3. TTL auto-deletes after 24h
// No manual cleanup needed!
```

## Use Case 2: User Sessions & Authentication

### Scenario

```
You're managing: Web app user sessions

Requirements:
â”œâ”€ Store session info (JWT refresh tokens)
â”œâ”€ Lookup session by token
â”œâ”€ Auto-expire sessions (1 hour)
â”œâ”€ Handle millions of sessions
â”œâ”€ Lightning-fast lookups
â””â”€ Secure (encrypted)

Why DynamoDB:
â”œâ”€ TTL: Sessions auto-expire
â”œâ”€ GSI: Lookup by token quickly
â”œâ”€ Sub-millisecond performance
â”œâ”€ Encryption at rest
â””â”€ Scales to millions of sessions
```

### Data Model

```
Table: Sessions
â”œâ”€ Partition Key: session_id
â”œâ”€ GSI: SessionsByUserId (lookup by user)
â”œâ”€ TTL: expires_at (1 hour)
â””â”€ Attributes:
   â”œâ”€ session_id (String) - Primary key
   â”œâ”€ user_id (String) - For GSI
   â”œâ”€ token (String) - JWT refresh token
   â”œâ”€ ip_address (String) - For security
   â”œâ”€ device (String) - Device info
   â”œâ”€ created_at (Number) - Timestamp
   â””â”€ expires_at (Number) - TTL attribute
```

### Operations

```javascript
// 1. Create session on login
await dynamodb.putItem({
  TableName: "Sessions",
  Item: {
    session_id: "sess-abc123",
    user_id: "user-100",
    token: "eyJhbGciOiJIUzI1NiIs...",
    ip_address: "192.0.2.1",
    created_at: Date.now(),
    expires_at: Math.floor(Date.now() / 1000) + 3600  // 1 hour from now
  }
});

// 2. Verify session (fast)
const session = await dynamodb.getItem({
  TableName: "Sessions",
  Key: { session_id: "sess-abc123" }
});

if (!session) {
  throw new Error("Session expired or invalid");
}

// 3. Sessions auto-delete after TTL
// No manual cleanup needed!
```

## Use Case 3: Real-Time Analytics & Metrics

### Scenario

```
You're tracking: Website analytics (page views, clicks)

Requirements:
â”œâ”€ Track events in real-time
â”œâ”€ Aggregate by page, country, device
â”œâ”€ Query "views per page today"
â”œâ”€ Handle 10K+ events/second
â”œâ”€ Variable traffic (mobile vs desktop)
â””â”€ Queries must be fast

Why DynamoDB:
â”œâ”€ Partition by metric (page, country)
â”œâ”€ Sort by timestamp (time-series)
â”œâ”€ On-demand pricing (variable traffic)
â”œâ”€ Can handle 10K+ writes/second
â””â”€ Query by date range easily
```

### Data Model

```
Table: PageViews
â”œâ”€ Partition Key: page_id
â”œâ”€ Sort Key: timestamp
â”œâ”€ Attributes:
   â”œâ”€ page_id: "/products"
   â”œâ”€ timestamp: 1706461200000
   â”œâ”€ country: "US"
   â”œâ”€ device: "mobile"
   â”œâ”€ session_id: "sess-xyz"
   â””â”€ referrer: "google"

Queries:
1. Get page views for /products today
   â”œâ”€ Partition: page_id = "/products"
   â”œâ”€ Sort: timestamp >= today
   â””â”€ Result: All views for that page today

2. Aggregate views by country
   â”œâ”€ Scan filtered by date range
   â”œâ”€ Group by country in application
   â””â”€ Result: Views per country
```

### Cost Example

```
Analytics volume:
â”œâ”€ Events: 10K per second = 864M per day
â”œâ”€ Storage: 5TB per month
â””â”€ On-demand: Pay per request

Monthly cost:
â”œâ”€ Writes: 864M Ã— $1.25/1M = $1,080
â”œâ”€ Reads: 100M queries Ã— $0.25/1M = $25
â”œâ”€ Storage: 5TB Ã— $0.25/GB = $1,280
â””â”€ Total: ~$2,385/month

Alternative (Analytics DB):
â”œâ”€ Would cost 3-5x more
â””â”€ DynamoDB wins for this use case
```

## Use Case 4: Real-Time Notifications

### Scenario

```
You're managing: User notifications

Requirements:
â”œâ”€ Store notifications per user
â”œâ”€ Mark as read/unread
â”œâ”€ Query unread count
â”œâ”€ Delete old notifications (30 days)
â”œâ”€ Handle 1M+ concurrent users
â””â”€ Real-time updates

Why DynamoDB:
â”œâ”€ Partition by user_id
â”œâ”€ Sort by timestamp (newest first)
â”œâ”€ TTL: Auto-delete 30+ day old
â”œâ”€ Update in real-time
â””â”€ Can handle 1M+ concurrent
```

### Data Model

```
Table: Notifications
â”œâ”€ Partition Key: user_id
â”œâ”€ Sort Key: notification_id
â”œâ”€ TTL: created_at + 30 days
â””â”€ Attributes:
   â”œâ”€ user_id: "user-100"
   â”œâ”€ notification_id: "notif-123"
   â”œâ”€ message: "Alice liked your post"
   â”œâ”€ type: "like" | "comment" | "follow"
   â”œâ”€ is_read: false
   â”œâ”€ created_at: 1706461200
   â””â”€ link: "/posts/123"

Queries:
1. Get all unread notifications
   â””â”€ Query user_id, filter is_read = false

2. Count unread
   â””â”€ Query user_id with limit = 1000
   â””â”€ Count filter results

3. Mark as read
   â””â”€ UpdateItem on notification_id
```

## Use Case 5: Time-Series Data

### Scenario

```
You're tracking: IoT sensor data (temperature, humidity)

Requirements:
â”œâ”€ Store readings from 1000s of sensors
â”œâ”€ One reading per sensor per minute
â”œâ”€ Query readings for specific sensor + date range
â”œâ”€ Store years of historical data
â”œâ”€ Archive old data (older than 1 year)
â””â”€ Fast time-range queries

Why DynamoDB:
â”œâ”€ Partition by sensor_id
â”œâ”€ Sort by timestamp (time-series)
â”œâ”€ Range queries: "Get sensor-1 readings from yesterday"
â”œâ”€ TTL: Auto-archive to Glacier
â””â”€ Can store years of data cost-effectively
```

### Data Model

```
Table: SensorReadings
â”œâ”€ Partition Key: sensor_id
â”œâ”€ Sort Key: timestamp
â”œâ”€ TTL: auto-archive after 1 year
â””â”€ Attributes:
   â”œâ”€ sensor_id: "sensor-temp-001"
   â”œâ”€ timestamp: 1706461200
   â”œâ”€ temperature: 22.5
   â”œâ”€ humidity: 45.2
   â”œâ”€ pressure: 1013.25
   â””â”€ location: "warehouse-1"

Queries:
1. Get all readings for sensor-1 today
   â”œâ”€ Partition: sensor_id = "sensor-temp-001"
   â”œâ”€ Sort: timestamp >= today midnight
   â””â”€ Result: All today's readings (fast!)

2. Get readings for specific hour
   â”œâ”€ Partition: sensor_id
   â”œâ”€ Sort: timestamp BETWEEN 14:00-15:00
   â””â”€ Result: All readings in that hour
```

## Use Case 6: User Profiles & Preferences

### Scenario

```
You're storing: User profiles + preferences

Requirements:
â”œâ”€ Store profile info (name, bio, settings)
â”œâ”€ Variable attributes (some users have more data)
â”œâ”€ Query by user_id (fast)
â”œâ”€ Update preferences frequently
â”œâ”€ Flexible schema (new features over time)
â””â”€ Real-time updates

Why DynamoDB:
â”œâ”€ Partition by user_id
â”œâ”€ Flexible schema (add attributes anytime)
â”œâ”€ Fast gets by user_id
â”œâ”€ Update individual fields
â””â”€ Scales to 100M+ users
```

### Data Model

```
Table: UserProfiles
â”œâ”€ Partition Key: user_id
â””â”€ Attributes:
   â”œâ”€ user_id: "user-100"
   â”œâ”€ email: "alice@example.com"
   â”œâ”€ username: "alice_j"
   â”œâ”€ profile_pic_url: "s3://bucket/pic.jpg"
   â”œâ”€ bio: "Software engineer, coffee lover"
   â”œâ”€ preferences: {
   â”‚  â”œâ”€ theme: "dark"
   â”‚  â”œâ”€ notifications: true
   â”‚  â”œâ”€ newsletter: false
   â”‚  â””â”€ language: "en"
   â”‚}
   â”œâ”€ social_accounts: {
   â”‚  â”œâ”€ github: "alice_j"
   â”‚  â”œâ”€ twitter: "alice_codes"
   â”‚  â””â”€ linkedin: "alice-j"
   â”‚}
   â”œâ”€ stats: {
   â”‚  â”œâ”€ followers: 1250
   â”‚  â”œâ”€ posts: 87
   â”‚  â””â”€ joined_at: 1704067200
   â”‚}
   â””â”€ updated_at: 1706470000
```

## When NOT to Use DynamoDB

```
âŒ Complex SQL queries
   â†’ Use: RDS (MySQL, PostgreSQL)

âŒ Complex joins across tables
   â†’ Use: RDS with multi-table queries

âŒ Strongly consistent reports
   â†’ Use: RDS or Redshift

âŒ Small datasets with infrequent access
   â†’ Use: Database or S3

âŒ Data analysis with SQL
   â†’ Use: Redshift or Athena

âŒ Full-text search
   â†’ Use: OpenSearch or Elasticsearch
```

## Best Practices

âœ… Design tables around access patterns
âœ… Use partition key + sort key for scalability
âœ… Use GSI for alternative access patterns
âœ… Avoid scans in production (use Query)
âœ… Use on-demand for unpredictable traffic
âœ… Use provisioned for predictable traffic
âœ… TTL for temporary data (auto-cleanup)
âœ… DynamoDB Streams for change data capture
âœ… Monitor with CloudWatch
âœ… Archive old data to S3

## Next Steps

â†’ [What is DynamoDB](./what-is-dynamodb.md) - Full overview
â†’ [Tables & Items](./tables-and-items.md) - Core concepts